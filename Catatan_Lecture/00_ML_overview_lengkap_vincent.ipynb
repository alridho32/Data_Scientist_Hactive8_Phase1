{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lM2xGtyMbaUS"
   },
   "source": [
    "# Hands On"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN6zZ_Ms5RkE"
   },
   "source": [
    "## 1 - Perkenalan\n",
    "\n",
    ">Bab pengenalan harus diisi dengan identitas, gambaran besar dataset yang digunakan, dan objective yang ingin dicapai.\n",
    "\n",
    "Nama:\n",
    "\n",
    "Batch:\n",
    "\n",
    "Problem Statement:  \n",
    "\n",
    "Objective:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZnEk82c5RkF"
   },
   "source": [
    "## 2 - Import Libraries\n",
    "\n",
    "> Cell pertama pada notebook harus berisi dan hanya berisi semua library yang digunakan dalam project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBTxcMAY5RkF"
   },
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "#import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import FE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "#import model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFyjKIUcIPcW"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFaSnACF5RkG"
   },
   "outputs": [],
   "source": [
    "!pip install feature_engine==1.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0zCH2z25RkG"
   },
   "source": [
    "## 3 - Data Loading\n",
    "\n",
    ">Bagian ini berisi proses penyiapan data sebelum dilakukan eksplorasi data lebih lanjut. Proses Data Loading dapat berupa memberi nama baru untuk setiap kolom, mengecek ukuran dataset, dll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zhv8kJya5RkG"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/Vincentim27/data/refs/heads/main/beverages.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0jhOZUH5RkG"
   },
   "outputs": [],
   "source": [
    "#Duplicate Dataset\n",
    "\n",
    "data_duplicate = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mv5AEwt5RkG"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbO8mjuU5RkG"
   },
   "outputs": [],
   "source": [
    "#check dataset - 2\n",
    "\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae-i6EXMqC2N"
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMpUSTNkoxgq"
   },
   "outputs": [],
   "source": [
    "# Menambahkan 5 data baru yang berisi missing value\n",
    "new_data = {'Product_ID':[1001,1002,1003,1004,1005]\n",
    "            'Sales_Volume_(L)':2000,\n",
    "            'Product_Category':'Water',\n",
    "            'Price_per_Liter_(IDR)':5000,\n",
    "            'Advertising_Spend_(USD)':10000,\n",
    "            'Number_of_Retailers':[300,100,200,250,50],\n",
    "            'Temperature_(Â°C)':[22.2,18.0,21.5,18.7,24.3],\n",
    "            'Holiday_Season':[1,0,0,1,1],\n",
    "            'Market_Share_(%)':2.2,\n",
    "            'Competitor_Price_per_Liter_(IDR)':7000,\n",
    "            }\n",
    "\n",
    "df2 = df2._append(new_data, ignore_index=True)\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdFnWInj5RkH"
   },
   "source": [
    "## 4 - Exploratory Data Analysis (EDA)\n",
    "\n",
    ">Bagian ini berisi eksplorasi data pada dataset diatas dengan menggunakan query, grouping, visualisasi sederhana, dan lain sebagainya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMuCHwdk5RkH"
   },
   "outputs": [],
   "source": [
    "# Create Histogram and Scatter plot\n",
    "\n",
    "plt.figure(figsize = (16,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.histplot(data['price'], kde = True, bins = 30)\n",
    "plt.title('Histogram of price')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.scatterplot(x = 'volume', y = 'margin', data= data)\n",
    "plt.title('volume Vs margin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmnbCcRX5RkH"
   },
   "source": [
    "Statement ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PWq_CiG5RkH"
   },
   "source": [
    "## 5 - Feature Engineering\n",
    "\n",
    "> Bagian ini berisi proses penyiapan data untuk proses pelatihan model, seperti pembagian data menjadi train-test, transformasi data (normalisasi, encoding, dll.), dan proses-proses lain yang dibutuhkan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_3wG4W85RkH"
   },
   "source": [
    "#### Split between X (Features) and y (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YICqvTLT5RkH"
   },
   "outputs": [],
   "source": [
    "#Splitting between 'X' and 'y'\n",
    "\n",
    "X = data.drop(['Holiday Season'], axis = 1)\n",
    "y = data['Holiday Season']\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnAceQd05RkH"
   },
   "source": [
    "### Splitting between Train-Set and Test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipYmr6Mw5RkH"
   },
   "outputs": [],
   "source": [
    "#Splitting between train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 40)\n",
    "print('Train Size: ', X_train.shape)\n",
    "print('Test Size: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZH67r36-GlW"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_xV-S0J5RkI"
   },
   "source": [
    "### Handling Missing Value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtcdDJal5RkI"
   },
   "outputs": [],
   "source": [
    "X_train.isnull().sum().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOX4xxKB5RkI"
   },
   "outputs": [],
   "source": [
    "X_test.isnull().sum().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBT_4Jf35RkI"
   },
   "outputs": [],
   "source": [
    "y_train.isnull().sum().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHgEX9GA5RkI"
   },
   "outputs": [],
   "source": [
    "y_test.isnull().sum().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aATgqOCHvAD5"
   },
   "outputs": [],
   "source": [
    "# Printing rows where is null\n",
    "null = df[df[''].isnull()]\n",
    "null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XutIsfyAvlHi"
   },
   "outputs": [],
   "source": [
    "# Check skewness\n",
    "X_train[''].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBWB43GMvsX4"
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define imputers\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Apply imputation to numeric columns\n",
    "X_train[['']] = median_imputer.fit_transform(X_train[['']])\n",
    "X_test[['']] = median_imputer.transform(X_test[['']])\n",
    "\n",
    "# Apply imputation to categorical columns\n",
    "X_train[['']] = mode_imputer.fit_transform(X_train[['']])\n",
    "X_test[['']] = mode_imputer.transform(X_test[['']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUTREvcbv4GE"
   },
   "outputs": [],
   "source": [
    "X_train.isnull().sum().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQ5ETHRXv4_e"
   },
   "outputs": [],
   "source": [
    "X_test.isnull().sum().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewbzrzBiv6K_"
   },
   "outputs": [],
   "source": [
    "y_train.isnull().sum().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRZ7CXrpv7WC"
   },
   "outputs": [],
   "source": [
    "y_test.isnull().sum().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afcUgqwg5RkH"
   },
   "source": [
    "### Handling Outlier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "My9UjTp6qnh-"
   },
   "outputs": [],
   "source": [
    "# Create function to check skewness\n",
    "def check_skewness(df, *column_names):\n",
    "    return {col: df[col].skew() for col in column_names if col in df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qakglvQYqo0C"
   },
   "outputs": [],
   "source": [
    "skewness_results = check_skewness(X_train, 'Sales_Volume_(L)','Price_per_Liter_(IDR)','Advertising_Spend_(USD)',\n",
    "            'Number_of_Retailers','Temperature_(Â°C)','Market_Share_(%)','Competitor_Price_per_Liter_(IDR)')\n",
    "\n",
    "# Print skewness\n",
    "for col, skewness in skewness_results.items():\n",
    "    print(f\"{col}: {skewness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ay7LGz7Er26w"
   },
   "outputs": [],
   "source": [
    "# Initialize empty lists for each skewness category\n",
    "normal_columns = []\n",
    "skewed_columns = []\n",
    "extreme_skewed_columns = []\n",
    "\n",
    "# Loop through the skewness values and categorize the columns\n",
    "for col, skewness in skewness_results.items():\n",
    "    if skewness < -1.0 or skewness > 1.0:\n",
    "        extreme_skewed_columns.append(col)\n",
    "    elif abs(skewness) <= 0.5:  #or -> -0.5 <= skewness <= 0.5\n",
    "        normal_columns.append(col)\n",
    "    else:\n",
    "        skewed_columns.append(col)\n",
    "\n",
    "# Print the columns in each category\n",
    "print(f\"Normal: {normal_columns}\\nSkewed: {skewed_columns}\\nExtreme Skewed: {extreme_skewed_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlk-klSIr7Th"
   },
   "outputs": [],
   "source": [
    "# Create function to calculate outlier percentages\n",
    "def calculate_outlier_percentages(df, columns, distance):\n",
    "    for variable in columns:\n",
    "        IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n",
    "        lower_boundary = df[variable].quantile(0.25) - (IQR * distance)\n",
    "        upper_boundary = df[variable].quantile(0.75) + (IQR * distance)\n",
    "\n",
    "        outliers = df[(df[variable] < lower_boundary) | (df[variable] > upper_boundary)]\n",
    "        outlier_percentage = len(outliers) / len(df) * 100\n",
    "\n",
    "        print('Percentage of outliers in {}: {:.2f}%'.format(variable, outlier_percentage))\n",
    "\n",
    "# Calcuate outlier percentages before handling\n",
    "calculate_outlier_percentages(X_train, skewed_columns, 1.5)\n",
    "calculate_outlier_percentages(X_train, extreme_skewed_columns, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBH_0_7OsMPC"
   },
   "outputs": [],
   "source": [
    "# Create a figure and two subplots side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Boxplot for skewed columns\n",
    "sns.boxplot(data=X_train[skewed_columns], orient=\"h\", ax=axes[0])\n",
    "axes[0].set_title(\"Boxplot for Skewed Columns\")\n",
    "axes[0].set_xlabel(\"Values\")\n",
    "axes[0].set_ylabel(\"Columns\")\n",
    "\n",
    "# Boxplot for extreme skewed columns\n",
    "sns.boxplot(data=X_train[extreme_skewed_columns], orient=\"h\", ax=axes[1])\n",
    "axes[1].set_title(\"Boxplot for Extreme Skewed Columns\")\n",
    "axes[1].set_xlabel(\"Values\")\n",
    "axes[1].set_ylabel(\"Columns\")\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6V676_INsTmc"
   },
   "outputs": [],
   "source": [
    "# Create function to apply winsorization\n",
    "def apply_winsorization(train, variables, capping_method='iqr', tail='both', fold=3):\n",
    "    winsoriser = Winsorizer(capping_method=capping_method, tail=tail, fold=fold, variables=variables)\n",
    "    train_capped = winsoriser.fit_transform(train)\n",
    "    return train_capped\n",
    "\n",
    "# Apply to X_train column\n",
    "X_train = apply_winsorization(X_train, skewed_columns, fold=1.5)\n",
    "X_train = apply_winsorization(X_train, extreme_skewed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emmSKHwNsiTo"
   },
   "outputs": [],
   "source": [
    "# Check the outliers after handling\n",
    "calculate_outlier_percentages(X_train, skewed_columns, 1.5)\n",
    "calculate_outlier_percentages(X_train, extreme_skewed_columns, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGt13Pqfsken"
   },
   "outputs": [],
   "source": [
    "# Create a figure and two subplots side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Boxplot for skewed columns\n",
    "sns.boxplot(data=X_train[skewed_columns], orient=\"h\", ax=axes[0])\n",
    "axes[0].set_title(\"Boxplot for Skewed Columns\")\n",
    "axes[0].set_xlabel(\"Values\")\n",
    "axes[0].set_ylabel(\"Columns\")\n",
    "\n",
    "# Boxplot for extreme skewed columns\n",
    "sns.boxplot(data=X_train[extreme_skewed_columns], orient=\"h\", whis=3, ax=axes[1])\n",
    "axes[1].set_title(\"Boxplot for Extreme Skewed Columns\")\n",
    "axes[1].set_xlabel(\"Values\")\n",
    "axes[1].set_ylabel(\"Columns\")\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk9URTES5RkI"
   },
   "source": [
    "### Feature Selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSEuzOxuoY5Y"
   },
   "outputs": [],
   "source": [
    "!pip install phik\n",
    "import phik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSNQiVmnsvlT"
   },
   "outputs": [],
   "source": [
    "# Concatenate X_train and y_train\n",
    "concat_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Show X_train\n",
    "concat_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vTlv6G9s9NZ"
   },
   "outputs": [],
   "source": [
    "# Create function to correlate variables with default\n",
    "def compute_phik_correlation(dataframe, columns):\n",
    "    subset = dataframe[columns]\n",
    "    correlation_matrix = subset.phik_matrix()\n",
    "    return correlation_matrix['default_payment_next_month']\n",
    "\n",
    "# Define the list of columns for each subset\n",
    "columns = ['Sales_Volume_(L)','Price_per_Liter_(IDR)','Advertising_Spend_(USD)','Product_Category',\n",
    "            'Number_of_Retailers','Temperature_(Â°C)','Market_Share_(%)','Competitor_Price_per_Liter_(IDR)']\n",
    "\n",
    "# Compute Phi-K correlation for each set of columns and print\n",
    "correlation = compute_phik_correlation(concat_train, columns)\n",
    "\n",
    "# Print result\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2aLNDyU5RkJ"
   },
   "outputs": [],
   "source": [
    "#Drop column that < 0.05\n",
    "X_train_cat.drop(['Sales_Volume_(L)','Price_per_Liter_(IDR)','Advertising_Spend_(USD)'], axis = 1, inplace = True)\n",
    "X_test_cat.drop(['Sales_Volume_(L)','Price_per_Liter_(IDR)','Advertising_Spend_(USD)'], axis = 1, inplace = True)\n",
    "X_train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40ROLbVAtzE2"
   },
   "outputs": [],
   "source": [
    "# Show columns\n",
    "print(X_train.columns)\n",
    "print(X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4z5OieM4uUxZ"
   },
   "source": [
    "### Split num col cat col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5K-SLSr3uQPx"
   },
   "outputs": [],
   "source": [
    "num_columns = X_train.select_dtypes(exclude=['object']).columns.tolist() # Cara1 - not recomended\n",
    "#cara 2:\n",
    "num_normal = []\n",
    "num_skew = []\n",
    "cat_encoded = []\n",
    "cat_ordinal = []\n",
    "cat_nominal = []\n",
    "print(f'Numerical normal columns:\\n{num_normal}')\n",
    "print(f'Numerical skew columns:\\n{num_skew}')\n",
    "print(f'Categorical encoded columns:\\n{cat_encoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9Ir-uhHyk35"
   },
   "outputs": [],
   "source": [
    "# Feature scaling using standard scaler\n",
    "scaler = StandardScaler()\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False) # sparse_output(atau sparse aja)=False utk mengganti -> `.toarray()`\n",
    "ord = OrdinalEncoder()\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('num',scaler,num_columns),\n",
    "        ('nom',ohe,cat_columns),\n",
    "        ('ord',ord, cat_ordinal)],\n",
    "    remainder='passthrough' # untuk categorical yg sdh di encode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0RldQwZHPQz"
   },
   "source": [
    "## 6. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scwvLcKrzgVy"
   },
   "source": [
    "### Membuat pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFI0e8Jcyq5_"
   },
   "outputs": [],
   "source": [
    "# Model Definition using pipeline\n",
    "\n",
    "pipe_log = make_pipeline(preprocess,LogisticRegression(max_iter=1000000))\n",
    "pipe_svc = make_pipeline(preprocess,SVC())\n",
    "pipe_dt = make_pipeline(preprocess,DecisionTreeClassifier(random_state=10))\n",
    "pipe_rf = make_pipeline(preprocess,RandomForestClassifier(random_state=10))\n",
    "pipe_knn = make_pipeline(preprocess,KNeighborsClassifier())\n",
    "pipe_nb = make_pipeline(preprocess,GaussianNB())\n",
    "pipe_ada = make_pipeline(preprocess,AdaBoostClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfilF9ofzdBk"
   },
   "source": [
    "### Cross validation untuk memilih best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvniRNsBytWh"
   },
   "outputs": [],
   "source": [
    "# setting kfold\n",
    "skfold = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "# Define Cross Validation for each model\n",
    "cv_log_model = cross_val_score(pipe_log, X_train, y_train, cv = skfold, scoring='f1', n_jobs=-1)\n",
    "cv_svm_model = cross_val_score(pipe_svc, X_train, y_train, cv = skfold, scoring='f1', n_jobs=-1)\n",
    "cv_dt_model = cross_val_score(pipe_dt, X_train, y_train, cv = skfold, scoring='f1', n_jobs=-1)\n",
    "cv_rf_model = cross_val_score(pipe_rf, X_train, y_train, cv = skfold, scoring='f1', n_jobs=-1)\n",
    "cv_knn_model = cross_val_score(pipe_knn, X_train, y_train, cv = skfold, scoring='f1', n_jobs=-1)\n",
    "cv_nb_model = cross_val_score(pipe_nb, X_train, y_train, cv = skfold, scoring='f1', n_jobs=-1)\n",
    "cv_ada_model = cross_val_score(pipe_ada, X_train, y_train, cv = skfold, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCfgKqCqy04Q"
   },
   "outputs": [],
   "source": [
    "# Finding Best Model based on Cross_Val_Score (mean)\n",
    "name_model = []\n",
    "cv_scores = 0\n",
    "for cv,name in zip([cv_log_model,cv_svm_model,cv_dt_model,cv_rf_model,cv_knn_model,cv_nb_model,cv_ada_model],\n",
    "                   ['log_model','svm_model','dt_model','rf_model','knn_model','nb_model','ada_model']):\n",
    "  print(name)\n",
    "  print('f1score - All - Cross Validation :', cv)\n",
    "  print('f1score - Mean - Cross Validation :', cv.mean())\n",
    "  print('f1score - std - Cross Validation :', cv.std())\n",
    "  print('f1score - Range of Test Set :', (cv.mean()-cv.std()), '-' , (cv.mean()+cv.std()))\n",
    "  print('-'*50)\n",
    "  if cv.mean() > cv_scores:\n",
    "    cv_scores = cv.mean()\n",
    "    name_model = name\n",
    "  else:\n",
    "    pass\n",
    "print('Best Model:', name_model)\n",
    "print('Cross Val Mean from Best Model:', cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2B_Fug3kzCYo"
   },
   "outputs": [],
   "source": [
    "# Fit pipeline on the training data\n",
    "pipe_<best_model>.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODcSgIs1zE0o"
   },
   "outputs": [],
   "source": [
    "# Get predictions for both training and test data\n",
    "y_pred_train = pipe_<best_model>.predict(X_train)\n",
    "y_pred_test = pipe_<best_model>.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8ryj6GlzYVU"
   },
   "source": [
    "### Model Evaluation before tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3OKQxfazUKO"
   },
   "outputs": [],
   "source": [
    "# Print recall score\n",
    "print('Recall Score - Train Set  : ', recall_score(y_resample, y_pred_train))\n",
    "print('Recall Score - Test Set   : ', recall_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdgq11zxzlMP"
   },
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot confusion matrix for training data\n",
    "train_matrix = ConfusionMatrixDisplay.from_estimator(pipe_rf, X_resample, y_resample, cmap='PuBu', ax=axes[0])\n",
    "train_matrix.ax_.set_title('Confusion Matrix - Training Data')\n",
    "\n",
    "# Plot confusion matrix for test data\n",
    "test_matrix = ConfusionMatrixDisplay.from_estimator(pipe_rf, X_test, y_test, cmap='PuBu', ax=axes[1])\n",
    "test_matrix.ax_.set_title('Confusion Matrix - Test Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRi3tv4Dzphz"
   },
   "source": [
    "### Simpan ke dalam table komparasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXpRiE7azo5d"
   },
   "outputs": [],
   "source": [
    "# Create function to create reports\n",
    "def performance_report(all_reports, y_resample, y_pred_train, y_test, y_pred_test, name):\n",
    "    # Calculate recall scores\n",
    "    score_reports = {\n",
    "        'Recall Train Set': recall_score(y_resample, y_pred_train),\n",
    "        'Recall Test Set': recall_score(y_test, y_pred_test),\n",
    "    }\n",
    "\n",
    "    # Calculate confusion matrices for train and test sets\n",
    "    cm_train = confusion_matrix(y_resample, y_pred_train)\n",
    "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "    # Extract false negatives from the confusion matrices and add to the report\n",
    "    score_reports['False Negative Train'] = cm_train[1, 0]\n",
    "    score_reports['False Negative Test'] = cm_test[1, 0]\n",
    "\n",
    "    # Store the report in the dictionary with the specified model name\n",
    "    all_reports[name] = score_reports\n",
    "    return all_reports\n",
    "\n",
    "all_reports = {}\n",
    "all_reports = performance_report(all_reports, y_resample, y_pred_train, y_test, y_pred_test, 'Random Forest without Tuning')\n",
    "\n",
    "pd.DataFrame(all_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ1aibSb0P4P"
   },
   "source": [
    "### Model tuning pakai `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9Esd2VXzxMb"
   },
   "outputs": [],
   "source": [
    "# misal rf best model\n",
    "# Set up the parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'classifier__n_estimators': [50, 100, 200, 300],\n",
    "    'classifier__max_depth': [1, 2, 3, 4],\n",
    "    'classifier__min_samples_split': [2, 3, 5, 7, 9],\n",
    "    'classifier__min_samples_leaf': [3, 5, 7, 9, 11],\n",
    "    'classifier__class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Set up the GridSearchCV object for Random Forest\n",
    "grid_search_rf = RandomSearchCV(pipe_rf,\n",
    "                              param_grid=param_grid_rf,\n",
    "                              scoring='recall',\n",
    "                              cv=kf,\n",
    "                              verbose=2,\n",
    "                              n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV for Random Forest\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters for Random Forest\n",
    "print('Best hyperparameters for Random Forest:', grid_search_rf.best_params_)\n",
    "\n",
    "# Best recall for Random Forest\n",
    "print('Best recall for Random Forest:', grid_search_rf.best_score_)\n",
    "\n",
    "# Save best Random Forest model to a variable\n",
    "best_rf_model = grid_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1mmgz9V0Zm6"
   },
   "source": [
    "### Model evaluation after tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyronOVnz8Ic"
   },
   "outputs": [],
   "source": [
    "# Get predictions for training and testing set using the hyperparameter tuned model\n",
    "y_pred_train_tuned = best_rf_model.predict(X_train)\n",
    "y_pred_test_tuned = best_rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_fhTSkCz_IG"
   },
   "outputs": [],
   "source": [
    "# Print recall score\n",
    "print('Recall Score - Train Set  : ', recall_score(y_resample, y_pred_train_tuned))\n",
    "print('Recall Score - Test Set   : ', recall_score(y_test, y_pred_test_tuned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPaeT3zD0Bij"
   },
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot confusion matrix for training data\n",
    "train_matrix = ConfusionMatrixDisplay.from_estimator(best_rf_model, X_resample, y_resample, cmap='PuBu', ax=axes[0])\n",
    "train_matrix.ax_.set_title('Confusion Matrix - Training Data')\n",
    "\n",
    "# Plot confusion matrix for test data\n",
    "test_matrix = ConfusionMatrixDisplay.from_estimator(best_rf_model, X_test, y_test, cmap='PuBu', ax=axes[1])\n",
    "test_matrix.ax_.set_title('Confusion Matrix - Test Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKoGLzI_0dGk"
   },
   "source": [
    "### Compare before-after tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qZU4dsV0FMp"
   },
   "outputs": [],
   "source": [
    "# Add results to the report\n",
    "all_reports = performance_report(all_reports, y_resample, y_pred_train_tuned, y_test, y_pred_test_tuned, 'Random Forest with Tuning')\n",
    "pd.DataFrame(all_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmd43p2a0LB1"
   },
   "source": [
    "### Model saving for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sHsKSMN0HJp"
   },
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "with open('best_rf_model.pkl', 'wb') as model_file:\n",
    "  pickle.dump(best_rf_model, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQPNk9gajRlM"
   },
   "source": [
    "---\n",
    "$$-- END --$$\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
